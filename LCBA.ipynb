{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1113d9c8",
   "metadata": {},
   "source": [
    "# Light-contact boxing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccbdea",
   "metadata": {},
   "source": [
    "# Import\n",
    "Import différentes bibliothèques et modules Python utilisés dans l'analyse de données et la visualisation. Voici une brève description de chaque import :\n",
    "\n",
    "- `pandas as pd` : Il importe la bibliothèque Pandas, qui est utilisée pour la manipulation et l'analyse de données tabulaires.\n",
    "- `numpy as np` : Il importe la bibliothèque NumPy, qui fournit des structures de données et des fonctions pour effectuer des calculs numériques efficaces.\n",
    "- `matplotlib.pyplot as plt` : Il importe la bibliothèque Matplotlib, qui est utilisée pour créer des visualisations graphiques, et spécifiquement le module `pyplot` pour les tracés de base.\n",
    "- `plotly.graph_objects as go` : Il importe la bibliothèque Plotly, qui permet de créer des visualisations interactives et dynamiques. L'importation de `graph_objects` permet d'utiliser les fonctionnalités avancées de Plotly.\n",
    "- `matplotlib.dates as mdates` : Il importe le module `dates` de Matplotlib, qui fournit des fonctionnalités pour travailler avec des données de type date et heure dans les tracés.\n",
    "- `scipy.stats as stats` : Il importe le module `stats` de SciPy, qui offre des fonctions statistiques et des distributions de probabilité.\n",
    "- `sklearn.ensemble import IsolationForest` : Il importe la classe `IsolationForest` de la bibliothèque scikit-learn, qui est utilisée pour détecter les anomalies dans les données à l'aide de l'algorithme de la forêt isolante.\n",
    "- `sklearn.cluster import DBSCAN` : Il importe la classe `DBSCAN` de scikit-learn, qui est utilisée pour effectuer le clustering basé sur la densité des données.\n",
    "- `sklearn import metrics` : Il importe le module `metrics` de scikit-learn, qui fournit des métriques pour évaluer la performance des modèles d'apprentissage automatique.\n",
    "- `sklearn.preprocessing import StandardScaler` : Il importe la classe `StandardScaler` de scikit-learn, qui est utilisée pour normaliser les données en les mettant à l'échelle.\n",
    "- `fastdtw` : Il importe la fonction `fastdtw` de la bibliothèque FastDTW, qui est utilisée pour calculer la distance DTW (Dynamic Time Warping) entre deux séries temporelles.\n",
    "- `scipy.spatial.distance import euclidean` : Il importe la fonction `euclidean` du module `distance` de SciPy, qui est utilisée pour calculer la distance euclidienne entre deux vecteurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2cce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.dates as mdates\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac59229",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f32eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('LCBA_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75163c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38783 entries, 0 to 38782\n",
      "Data columns (total 37 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   score_id                  38783 non-null  int64  \n",
      " 1   red_penalty               38783 non-null  int64  \n",
      " 2   red_point                 38783 non-null  int64  \n",
      " 3   blue_penalty              38783 non-null  int64  \n",
      " 4   blue_point                38783 non-null  int64  \n",
      " 5   date_create               38783 non-null  object \n",
      " 6   date_change               38783 non-null  object \n",
      " 7   judge_id                  38783 non-null  int64  \n",
      " 8   match_id                  38783 non-null  int64  \n",
      " 9   date_create_app           36786 non-null  object \n",
      " 10  uuid                      36786 non-null  object \n",
      " 11  judge_club_id             38783 non-null  int64  \n",
      " 12  match_id.1                38783 non-null  int64  \n",
      " 13  PalmaresDate              38783 non-null  object \n",
      " 14  winner                    38745 non-null  object \n",
      " 15  PalmaresRedCountRanking   38783 non-null  bool   \n",
      " 16  PalmaresBlueCountRanking  38783 non-null  bool   \n",
      " 17  PalmaresRound             1157 non-null   float64\n",
      " 18  PalmaresFightNr           38783 non-null  int64  \n",
      " 19  PalmaresFinal             38783 non-null  bool   \n",
      " 20  PalmaresDeleted           38783 non-null  bool   \n",
      " 21  PalmaresBlueClubID        38783 non-null  int64  \n",
      " 22  PalmaresBlueMemberID      38783 non-null  int64  \n",
      " 23  PalmaresRedClubID         38783 non-null  int64  \n",
      " 24  PalmaresRedMemberID       38783 non-null  int64  \n",
      " 25  PalmaresResultID          38745 non-null  float64\n",
      " 26  PalmaresTournamentID      38783 non-null  int64  \n",
      " 27  PalmaresTrophyRuleID      27484 non-null  float64\n",
      " 28  PalmaresPublished         38783 non-null  bool   \n",
      " 29  PalmaresRealEndTime       38701 non-null  object \n",
      " 30  PalmaresRealStartTime     38783 non-null  object \n",
      " 31  PalmaresRingID            38676 non-null  float64\n",
      " 32  blue_injured              38783 non-null  bool   \n",
      " 33  open_time                 38783 non-null  object \n",
      " 34  red_injured               38783 non-null  bool   \n",
      " 35  blue_height               38783 non-null  int64  \n",
      " 36  red_height                38783 non-null  int64  \n",
      "dtypes: bool(7), float64(4), int64(17), object(9)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35416e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de match_id distincts : 813\n"
     ]
    }
   ],
   "source": [
    "num_distinct_match_ids = data['match_id'].nunique()\n",
    "print(\"Nombre de match_id distincts :\", num_distinct_match_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e94cb0",
   "metadata": {},
   "source": [
    "#### Description détaillés des colonnes qui seront les plus utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b22f648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean         0.456102\n",
       "std          0.561632\n",
       "min         -3.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          3.000000\n",
       "Name: red_point, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.red_point.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab53288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean         0.098858\n",
       "std          0.359887\n",
       "min         -3.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          3.000000\n",
       "Name: red_penalty, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.red_penalty.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5cd89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean         0.457288\n",
       "std          0.556790\n",
       "min         -3.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          3.000000\n",
       "Name: blue_point, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blue_point.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73379b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean         0.106825\n",
       "std          0.380282\n",
       "min         -3.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          3.000000\n",
       "Name: blue_penalty, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blue_penalty.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1c55eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                          36786\n",
       "unique                         36782\n",
       "top       2022-06-18 12:36:59.029+00\n",
       "freq                               2\n",
       "Name: date_create_app, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.date_create_app.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e9d5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     38783.000000\n",
       "mean     113041.017688\n",
       "std        3890.411609\n",
       "min      100124.000000\n",
       "25%      113341.000000\n",
       "50%      114926.000000\n",
       "75%      114928.000000\n",
       "max      115285.000000\n",
       "Name: judge_id, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.judge_id.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1e58e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean     21845.530645\n",
       "std        401.528301\n",
       "min      21089.000000\n",
       "25%      21487.000000\n",
       "50%      21923.000000\n",
       "75%      22202.000000\n",
       "max      22515.000000\n",
       "Name: match_id, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.match_id.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ccae3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     38745\n",
       "unique        4\n",
       "top        blue\n",
       "freq      17296\n",
       "Name: winner, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.winner.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ae1cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean       109.258979\n",
       "std         42.095726\n",
       "min          4.000000\n",
       "25%        101.000000\n",
       "50%        118.000000\n",
       "75%        138.000000\n",
       "max        213.000000\n",
       "Name: judge_club_id, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.judge_club_id.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd8b5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean        93.819354\n",
       "std         58.227887\n",
       "min          4.000000\n",
       "25%         47.000000\n",
       "50%        101.000000\n",
       "75%        154.000000\n",
       "max        213.000000\n",
       "Name: PalmaresBlueClubID, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.PalmaresBlueClubID.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4e104d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38783.000000\n",
       "mean       100.609004\n",
       "std         58.236669\n",
       "min          4.000000\n",
       "25%         67.000000\n",
       "50%        101.000000\n",
       "75%        155.000000\n",
       "max        213.000000\n",
       "Name: PalmaresRedClubID, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.PalmaresRedClubID.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8013ee6",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6b503",
   "metadata": {},
   "source": [
    "### Conversion date\n",
    "\n",
    "Ce code convertit les colonnes spécifiées dans la liste `cols_to_convert` en objets de type datetime à l'aide de la fonction `pd.to_datetime` de la bibliothèque Pandas. \n",
    "\n",
    "La méthode `apply` est utilisée pour appliquer la fonction `pd.to_datetime` à chaque colonne spécifiée dans `cols_to_convert`. Cela permet de convertir les valeurs de ces colonnes en objets de type datetime. Une fois les colonnes converties, les nouvelles valeurs sont assignées aux colonnes correspondantes dans le DataFrame `data`.\n",
    "\n",
    "En résumé, ce code convertit les colonnes spécifiées en objets de type datetime, ce qui facilite la manipulation et l'analyse des données temporelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929a97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_convert = ['date_create', 'date_change','date_create_app','PalmaresDate','PalmaresRealEndTime','PalmaresRealStartTime','open_time']\n",
    "data[cols_to_convert] = data[cols_to_convert].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09a109",
   "metadata": {},
   "source": [
    "### Suppresion des colonnes nulles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714ece2",
   "metadata": {},
   "source": [
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il sélectionne les lignes du DataFrame `data` pour lesquelles la colonne 'date_create_app' a une valeur nulle (NaN) et les assigne à la variable `null_rows`. Cela permet de filtrer les lignes qui ont une valeur manquante dans la colonne 'date_create_app'.\n",
    "\n",
    "2. Il crée une liste `cols_to_display2` qui contient les noms des colonnes que l'on souhaite afficher dans le résultat.\n",
    "\n",
    "3. Il affiche les colonnes spécifiées (`score_id`, `red_point`, `blue_point`, `judge_id`, `date_create_app`) des lignes où la colonne 'date_create_app' a une valeur nulle. Cela permet d'afficher les informations des lignes avec des valeurs manquantes dans la colonne 'date_create_app'.\n",
    "\n",
    "4. Ensuite, il utilise la méthode `dropna` du DataFrame pour supprimer les lignes qui ont une valeur manquante dans la colonne 'date_create_app'. Cela modifie le DataFrame `data` en supprimant les lignes avec des valeurs nulles dans la colonne spécifiée.\n",
    "\n",
    "En résumé, ce code identifie et affiche les lignes du DataFrame `data` qui ont une valeur manquante dans la colonne 'date_create_app', puis supprime ces lignes du DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc87ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       score_id  red_point  blue_point  judge_id date_create_app\n",
      "0             1          0           0    114816             NaT\n",
      "1             2          0           0    114822             NaT\n",
      "2             3          0           0    114824             NaT\n",
      "3             4          0           0    114826             NaT\n",
      "4             5          0           0    114813             NaT\n",
      "...         ...        ...         ...       ...             ...\n",
      "38639     38813          0           0    114927             NaT\n",
      "38640     38814          0           0    114926             NaT\n",
      "38711     38885          0           0    114927             NaT\n",
      "38712     38886          0           0    114926             NaT\n",
      "38713     38887          0           0    115226             NaT\n",
      "\n",
      "[1997 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "null_rows = data[data['date_create_app'].isnull()]\n",
    "\n",
    "cols_to_display2 = ['score_id', 'red_point', 'blue_point', 'judge_id', 'date_create_app']\n",
    "print(null_rows[cols_to_display2])\n",
    "\n",
    "# Supprimer les lignes\n",
    "data = data.dropna(subset=['date_create_app'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080323f",
   "metadata": {},
   "source": [
    "### Correction des points\n",
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il met à jour les valeurs de la colonne 'blue_point' à 0 pour les lignes où la colonne 'red_penalty' est différente de 0. Cela signifie que si la valeur de 'red_penalty' est différente de 0, la valeur de 'blue_point' sera mise à 0 pour cette ligne.\n",
    "\n",
    "2. Il met à jour les valeurs de la colonne 'red_point' à 0 pour les lignes où la colonne 'blue_penalty' est différente de 0. Cela signifie que si la valeur de 'blue_penalty' est différente de 0, la valeur de 'red_point' sera mise à 0 pour cette ligne.\n",
    "\n",
    "3. Il compte le nombre de lignes modifiées en comptant le nombre de lignes où soit la colonne 'red_penalty' est différente de 0, soit la colonne 'blue_penalty' est différente de 0. Cela compte le nombre de lignes où au moins l'une des colonnes de pénalité a une valeur différente de 0.\n",
    "\n",
    "4. Enfin, il affiche le nombre de lignes modifiées.\n",
    "\n",
    "En résumé, ce code met à jour les valeurs des colonnes 'blue_point' et 'red_point' en fonction des valeurs des colonnes de pénalité ('red_penalty' et 'blue_penalty'). Il compte ensuite le nombre de lignes modifiées et l'affiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4691a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes modifiées : 7805\n"
     ]
    }
   ],
   "source": [
    "# Mettre à jour les lignes avec des pénalités\n",
    "data.loc[data['red_penalty'] != 0, 'blue_point'] = 0\n",
    "data.loc[data['blue_penalty'] != 0, 'red_point'] = 0\n",
    "\n",
    "# Compter le nombre de lignes modifiées\n",
    "num_modified_rows = sum((data['red_penalty'] != 0) | (data['blue_penalty'] != 0))\n",
    "\n",
    "print(\"Nombre de lignes modifiées :\", num_modified_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80580ca",
   "metadata": {},
   "source": [
    "### Correction user\n",
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il compte le nombre de lignes où la valeur de la colonne 'red_point' est négative en utilisant la condition `data['red_point'] < 0`. La fonction `sum` est utilisée pour compter le nombre de lignes satisfaisant cette condition.\n",
    "\n",
    "2. Il compte le nombre de lignes où la valeur de la colonne 'blue_point' est négative en utilisant la condition `data['blue_point'] < 0`. La fonction `sum` est utilisée pour compter le nombre de lignes satisfaisant cette condition.\n",
    "\n",
    "3. Ensuite, il affiche le nombre de lignes avec des valeurs négatives dans la colonne 'red_point' et le nombre de lignes avec des valeurs négatives dans la colonne 'blue_point'.\n",
    "\n",
    "En résumé, ce code compte le nombre de lignes dans le DataFrame 'data' où les colonnes 'red_point' et 'blue_point' ont des valeurs négatives, puis affiche ces nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e80b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avec red_point négatif : 352\n",
      "Nombre de lignes avec blue_point négatif : 415\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de lignes négatives dans red_point et blue_point\n",
    "num_negative_red_points = sum(data['red_point'] < 0)\n",
    "num_negative_blue_points = sum(data['blue_point'] < 0)\n",
    "\n",
    "print(\"Nombre de lignes avec red_point négatif :\", num_negative_red_points)\n",
    "print(\"Nombre de lignes avec blue_point négatif :\", num_negative_blue_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723c02e",
   "metadata": {},
   "source": [
    "Ce code permet de supprimer les lignes qui ont une valeur négative dans les colonnes 'red_point' ou 'blue_point' tout en conservant la ligne la plus proche en termes de temps (utilisant la colonne 'date_create_app') pour chaque combinaison de 'judge_id' et 'match_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb686a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes supprimées : 1534\n"
     ]
    }
   ],
   "source": [
    "# Convertir la colonne 'date_create_app' en datetime\n",
    "data['date_create_app'] = pd.to_datetime(data['date_create_app'])\n",
    "\n",
    "# Compter le nombre de lignes supprimées\n",
    "num_deleted_rows = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['red_point'] < 0 or row['blue_point'] < 0:\n",
    "        judge_id = row['judge_id']\n",
    "        match_id = row['match_id']\n",
    "        current_timestamp = row['date_create_app']\n",
    "\n",
    "        # Rechercher les lignes avec le même judge_id et match_id\n",
    "        same_judge_id_rows = data[(data['judge_id'] == judge_id) & (data['match_id'] == match_id)]\n",
    "\n",
    "        if len(same_judge_id_rows) > 1:\n",
    "            # Calculer la différence de temps avec chaque ligne\n",
    "            time_diff = abs(same_judge_id_rows['date_create_app'] - current_timestamp)\n",
    "\n",
    "            # Trouver l'index de la ligne la plus proche (à l'exception de la ligne actuelle)\n",
    "            closest_index = time_diff[time_diff.index != index].idxmin()\n",
    "\n",
    "            # Supprimer la ligne la plus proche (si elle existe)\n",
    "            data = data.drop(closest_index, errors='ignore')\n",
    "            num_deleted_rows += 1\n",
    "\n",
    "        # Supprimer la ligne actuelle\n",
    "        data = data.drop(index, errors='ignore')\n",
    "        num_deleted_rows += 1\n",
    "\n",
    "print(\"Nombre de lignes supprimées :\", num_deleted_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef27510",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81b4e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     match_id  red_point  blue_point\n",
      "0       21089         20          15\n",
      "1       21092          9          22\n",
      "2       21093         19          32\n",
      "3       21095         31          25\n",
      "4       21096         35          27\n",
      "..        ...        ...         ...\n",
      "803     22511          8          23\n",
      "804     22512          4          10\n",
      "805     22513         40          23\n",
      "806     22514         22          26\n",
      "807     22515         25          25\n",
      "\n",
      "[808 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "total_points = data.groupby(\"match_id\")[[\"red_point\", \"blue_point\"]].sum().reset_index()\n",
    "\n",
    "print(total_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b8e8f",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf4c65",
   "metadata": {},
   "source": [
    "### Détection des rounds\n",
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il définit une liste `match_ids` contenant les identifiants uniques des matchs dans la colonne 'match_id' du DataFrame 'data'.\n",
    "\n",
    "2. Il crée une nouvelle colonne 'round' dans le DataFrame 'data' pour stocker les valeurs du round.\n",
    "\n",
    "3. Il définit une liste 'outliers' pour stocker les indices des lignes identifiées comme des outliers.\n",
    "\n",
    "4. Il itère sur chaque match_id dans la liste 'match_ids'.\n",
    "\n",
    "5. Pour chaque match_id, il sélectionne les lignes correspondant à ce match_id et les trie par ordre chronologique dans la colonne 'date_create_app'.\n",
    "\n",
    "6. Il convertit la colonne 'date_create_app' en valeurs numériques (timestamps) pour être utilisée dans le clustering.\n",
    "\n",
    "7. Il effectue le clustering DBSCAN sur les valeurs normalisées des timestamps.\n",
    "\n",
    "8. Il crée une colonne 'round' dans le sous-dataframe 'match_data' pour stocker les numéros de round attribués par le clustering.\n",
    "\n",
    "9. Il met à jour la colonne 'round' dans le DataFrame 'data' en utilisant la méthode `.loc`.\n",
    "\n",
    "10. Il stocke les indices des outliers détectés dans le sous-dataframe 'match_outliers' dans la liste 'outliers'.\n",
    "\n",
    "11. Après avoir itéré sur tous les match_ids, il ajoute une colonne 'outlier' dans le DataFrame 'data' et attribue la valeur True aux lignes identifiées comme outliers.\n",
    "\n",
    "12. Enfin, il affiche les colonnes 'score_id', 'match_id', 'date_create_app', 'round' et 'outlier' du DataFrame 'data'.\n",
    "\n",
    "En résumé, ce code effectue le clustering des matchs en utilisant la colonne 'date_create_app' comme caractéristique temporelle, attribue un numéro de round à chaque match basé sur le clustering et identifie les outliers dans les données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4849232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       score_id  match_id                  date_create_app round outlier\n",
      "5             6     21089 2021-06-26 11:14:51.610000+00:00     1     NaN\n",
      "6             7     21089 2021-06-26 11:14:48.802000+00:00     1     NaN\n",
      "7             8     21089 2021-06-26 11:14:53.035000+00:00     1     NaN\n",
      "8             9     21089 2021-06-26 11:14:53.541000+00:00     1     NaN\n",
      "9            10     21093 2021-06-26 11:14:52.862000+00:00     1     NaN\n",
      "...         ...       ...                              ...   ...     ...\n",
      "38778     38952     22514 2023-04-15 14:18:52.200000+00:00     3     NaN\n",
      "38779     38953     22514 2023-04-15 14:18:51.485000+00:00     3     NaN\n",
      "38780     38954     22514 2023-04-15 14:18:56.330000+00:00     3     NaN\n",
      "38781     38955     22514 2023-04-15 14:18:56.889000+00:00     3     NaN\n",
      "38782     38956     22514 2023-04-15 14:19:01.423000+00:00     3     NaN\n",
      "\n",
      "[35321 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Définir la liste des match_ids\n",
    "match_ids = data['match_id'].unique()\n",
    "\n",
    "# Créer une nouvelle colonne pour stocker les valeurs de round\n",
    "data['round'] = None\n",
    "\n",
    "# Définir la liste des outliers\n",
    "outliers = []\n",
    "\n",
    "# Parcourir tous les match_ids\n",
    "for match_id in match_ids:\n",
    "    # Sélectionner les lignes correspondant au match_id actuel\n",
    "    match_data = data[data['match_id'] == match_id].copy()  # Fait une copie du sous-dataframe\n",
    "\n",
    "    # Trier les valeurs par ordre chronologique dans la colonne \"date_create_app\"\n",
    "    match_data.sort_values(by='date_create_app', inplace=True)\n",
    "\n",
    "    # Conversion de la colonne \"date_create_app\" en valeurs numériques\n",
    "    match_data['timestamp'] = match_data['date_create_app'].apply(lambda x: pd.to_datetime(x).timestamp())\n",
    "\n",
    "    # Sélection des colonnes à utiliser pour le clustering\n",
    "    features = match_data[['timestamp']]\n",
    "\n",
    "    # Normalisation des caractéristiques\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Application de DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    labels = dbscan.fit_predict(normalized_features)\n",
    "\n",
    "    # Création de la colonne \"round\" pour stocker le numéro du round\n",
    "    match_data['round'] = labels + 1\n",
    "\n",
    "    # Mise à jour de la colonne \"round\" dans data en utilisant .loc\n",
    "    data.loc[data['match_id'] == match_id, 'round'] = match_data['round'].values\n",
    "\n",
    "    # Stocker les indices des outliers dans le tableau outliers\n",
    "    match_outliers = match_data[labels == -1]\n",
    "    outliers.extend(match_outliers.index.tolist())\n",
    "\n",
    "# Ajouter une colonne \"outlier\" avec la valeur \"True\" pour les lignes détectées comme outliers dans data\n",
    "data.loc[outliers, 'outlier'] = True\n",
    "\n",
    "# Affichage du résultat\n",
    "print(data[['score_id', 'match_id', 'date_create_app', 'round', 'outlier']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bb7f0",
   "metadata": {},
   "source": [
    "### Détection des phases\n",
    "\n",
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il crée une nouvelle colonne 'phase' dans le DataFrame 'data' pour stocker les clusters de phases.\n",
    "\n",
    "2. Il itère sur chaque match_id dans la liste 'match_ids'.\n",
    "\n",
    "3. Pour chaque match_id, il sélectionne les lignes correspondant à ce match_id et les copie dans le sous-dataframe 'match_data'.\n",
    "\n",
    "4. Il obtient les rounds distincts pour le match_id actuel.\n",
    "\n",
    "5. Il itère sur chaque round dans la liste des rounds.\n",
    "\n",
    "6. Pour chaque round, il sélectionne les lignes correspondant à ce round dans le sous-dataframe 'match_data' et les copie dans le sous-dataframe 'round_data'.\n",
    "\n",
    "7. Il trie les valeurs par ordre chronologique dans la colonne 'date_create_app' du sous-dataframe 'round_data'.\n",
    "\n",
    "8. Il convertit la colonne 'date_create_app' en valeurs numériques (timestamps) pour être utilisée dans le clustering.\n",
    "\n",
    "9. Il effectue le clustering DBSCAN sur les valeurs normalisées des timestamps sans la détection d'outliers (min_samples=1).\n",
    "\n",
    "10. Il ajuste les étiquettes de cluster pour commencer à 1 au lieu de 0.\n",
    "\n",
    "11. Il crée une colonne 'cluster' dans le sous-dataframe 'round_data' pour stocker les clusters détectés.\n",
    "\n",
    "12. Il met à jour la colonne 'phase' dans le DataFrame 'data' en utilisant la méthode `.loc`.\n",
    "\n",
    "13. Après avoir itéré sur tous les rounds d'un match_id, il passe au match_id suivant.\n",
    "\n",
    "14. Enfin, il affiche les colonnes 'score_id', 'match_id', 'date_create_app', 'round' et 'phase' du DataFrame 'data'.\n",
    "\n",
    "En résumé, ce code effectue le clustering des phases dans chaque round d'un match en utilisant la colonne 'date_create_app' comme caractéristique temporelle. Chaque phase est attribuée à un cluster distinct, et ces informations sont stockées dans la colonne 'phase' du DataFrame 'data'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98ce538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       score_id  match_id                  date_create_app round phase\n",
      "5             6     21089 2021-06-26 11:14:51.610000+00:00     1     1\n",
      "6             7     21089 2021-06-26 11:14:48.802000+00:00     1     1\n",
      "7             8     21089 2021-06-26 11:14:53.035000+00:00     1     1\n",
      "8             9     21089 2021-06-26 11:14:53.541000+00:00     1     1\n",
      "9            10     21093 2021-06-26 11:14:52.862000+00:00     1     1\n",
      "...         ...       ...                              ...   ...   ...\n",
      "38778     38952     22514 2023-04-15 14:18:52.200000+00:00     3     2\n",
      "38779     38953     22514 2023-04-15 14:18:51.485000+00:00     3     2\n",
      "38780     38954     22514 2023-04-15 14:18:56.330000+00:00     3     2\n",
      "38781     38955     22514 2023-04-15 14:18:56.889000+00:00     3     2\n",
      "38782     38956     22514 2023-04-15 14:19:01.423000+00:00     3     2\n",
      "\n",
      "[35321 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Détection des phases dans les rounds\n",
    "data['phase'] = None\n",
    "\n",
    "# Parcourir tous les match_ids\n",
    "for match_id in match_ids:\n",
    "    # Sélectionner les lignes correspondant au match_id actuel\n",
    "    match_data = data[data['match_id'] == match_id].copy()\n",
    "\n",
    "    # Obtenir les rounds distincts pour le match_id actuel\n",
    "    rounds = match_data['round'].unique()\n",
    "\n",
    "    # Parcourir tous les rounds\n",
    "    for round_val in rounds:\n",
    "        # Trier les valeurs par ordre chronologique dans la colonne \"date_create_app\"\n",
    "        round_data = match_data[match_data['round'] == round_val].copy()\n",
    "        round_data.sort_values(by='date_create_app', inplace=True)\n",
    "\n",
    "        # Conversion de la colonne \"date_create_app\" en valeurs numériques\n",
    "        round_data['timestamp'] = round_data['date_create_app'].apply(lambda x: pd.to_datetime(x).timestamp())\n",
    "\n",
    "        # Sélection des colonnes à utiliser pour le clustering\n",
    "        features = round_data[['timestamp']]\n",
    "\n",
    "        # Normalisation des caractéristiques\n",
    "        scaler = StandardScaler()\n",
    "        normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "        # Application de DBSCAN sans détection d'outliers\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=1)  # min_samples > 0 pour désactiver la détection d'outliers\n",
    "        labels = dbscan.fit_predict(normalized_features)\n",
    "\n",
    "        # Ajuster les étiquettes de cluster pour commencer à 1 au lieu de 0\n",
    "        labels += 1\n",
    "\n",
    "        # Création de la colonne \"phase\" pour stocker les clusters détectés\n",
    "        round_data['cluster'] = labels\n",
    "\n",
    "        # Mise à jour de la colonne \"phase\" dans le dataframe d'origine en utilisant .loc\n",
    "        data.loc[(data['match_id'] == match_id) & (data['round'] == round_val), 'phase'] = round_data['cluster'].values\n",
    "\n",
    "# Affichage du résultat\n",
    "print(data[['score_id', 'match_id', 'date_create_app', 'round', 'phase']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a2bb0",
   "metadata": {},
   "source": [
    "## DTW\n",
    "\n",
    "### Détéction des échanges\n",
    "\n",
    "Ce code effectue le filtrage des données en utilisant plusieurs conditions :\n",
    "\n",
    "1. Il crée un nouveau DataFrame appelé 'dataset' en filtrant les lignes du DataFrame 'data' qui satisfont les conditions suivantes :\n",
    "   - Les colonnes 'blue_point' et 'red_point' doivent être supérieures ou égales à zéro.\n",
    "   - Les colonnes 'red_penalty' et 'blue_penalty' doivent être égales à zéro.\n",
    "\n",
    "2. Il crée un nouveau DataFrame appelé 'filtered_data' en filtrant les lignes du DataFrame 'data' qui satisfont au moins une des conditions suivantes :\n",
    "   - La colonne 'blue_point' est inférieure à zéro.\n",
    "   - La colonne 'red_point' est inférieure à zéro.\n",
    "   - La colonne 'red_penalty' est supérieure à zéro.\n",
    "   - La colonne 'blue_penalty' est supérieure à zéro.\n",
    "\n",
    "3. Il affiche le résultat du DataFrame 'dataset' qui contient les lignes sans points négatifs dans 'blue_point' et 'red_point', ainsi que sans 'red_penalty' ou 'blue_penalty'.\n",
    "\n",
    "4. Il affiche le résultat du DataFrame 'filtered_data' qui contient les lignes avec des points négatifs dans 'blue_point' et 'red_point', ou avec 'red_penalty' ou 'blue_penalty'.\n",
    "\n",
    "En résumé, ce code sépare les données en deux ensembles : un ensemble contenant les lignes sans points négatifs ni pénalités, et un autre ensemble contenant les lignes avec des points négatifs ou des pénalités.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810b38f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sans points négatifs et pénalités:\n",
      "       score_id  red_penalty  red_point  blue_penalty  blue_point  \\\n",
      "5             6            0          0             0           1   \n",
      "6             7            0          0             0           1   \n",
      "7             8            0          1             0           0   \n",
      "8             9            0          0             0           1   \n",
      "9            10            0          0             0           1   \n",
      "...         ...          ...        ...           ...         ...   \n",
      "38778     38952            0          1             0           0   \n",
      "38779     38953            0          1             0           0   \n",
      "38780     38954            0          1             0           0   \n",
      "38781     38955            0          1             0           0   \n",
      "38782     38956            0          1             0           0   \n",
      "\n",
      "                           date_create                      date_change  \\\n",
      "5     2021-06-26 11:14:50.485437+00:00 2021-06-26 11:14:50.485449+00:00   \n",
      "6     2021-06-26 11:14:51.013705+00:00 2021-06-26 11:14:51.013715+00:00   \n",
      "7     2021-06-26 11:14:51.904174+00:00 2021-06-26 11:14:51.904189+00:00   \n",
      "8     2021-06-26 11:14:52.415167+00:00 2021-06-26 11:14:52.415180+00:00   \n",
      "9     2021-06-26 11:14:52.998925+00:00 2021-06-26 11:14:52.998941+00:00   \n",
      "...                                ...                              ...   \n",
      "38778 2023-04-15 14:18:51.638506+00:00 2023-04-15 14:18:51.638519+00:00   \n",
      "38779 2023-04-15 14:18:51.646201+00:00 2023-04-15 14:18:51.646215+00:00   \n",
      "38780 2023-04-15 14:18:56.476517+00:00 2023-04-15 14:18:56.476528+00:00   \n",
      "38781 2023-04-15 14:18:57.286122+00:00 2023-04-15 14:18:57.286132+00:00   \n",
      "38782 2023-04-15 14:19:00.882055+00:00 2023-04-15 14:19:00.882072+00:00   \n",
      "\n",
      "       judge_id  match_id                  date_create_app  ...  \\\n",
      "5        114816     21089 2021-06-26 11:14:51.610000+00:00  ...   \n",
      "6        114822     21089 2021-06-26 11:14:48.802000+00:00  ...   \n",
      "7        114816     21089 2021-06-26 11:14:53.035000+00:00  ...   \n",
      "8        114816     21089 2021-06-26 11:14:53.541000+00:00  ...   \n",
      "9        114826     21093 2021-06-26 11:14:52.862000+00:00  ...   \n",
      "...         ...       ...                              ...  ...   \n",
      "38778    115226     22514 2023-04-15 14:18:52.200000+00:00  ...   \n",
      "38779    114927     22514 2023-04-15 14:18:51.485000+00:00  ...   \n",
      "38780    114927     22514 2023-04-15 14:18:56.330000+00:00  ...   \n",
      "38781    114926     22514 2023-04-15 14:18:56.889000+00:00  ...   \n",
      "38782    115226     22514 2023-04-15 14:19:01.423000+00:00  ...   \n",
      "\n",
      "      PalmaresRealStartTime  PalmaresRingID  blue_injured           open_time  \\\n",
      "5       2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "6       2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "7       2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "8       2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "9       2023-06-19 11:14:36             2.0         False 2023-06-19 11:12:37   \n",
      "...                     ...             ...           ...                 ...   \n",
      "38778   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38779   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38780   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38781   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38782   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "\n",
      "      red_injured  blue_height  red_height  round  outlier  phase  \n",
      "5           False          182         179      1      NaN      1  \n",
      "6           False          182         179      1      NaN      1  \n",
      "7           False          182         179      1      NaN      1  \n",
      "8           False          182         179      1      NaN      1  \n",
      "9           False          167         165      1      NaN      1  \n",
      "...           ...          ...         ...    ...      ...    ...  \n",
      "38778       False          142         138      3      NaN      2  \n",
      "38779       False          142         138      3      NaN      2  \n",
      "38780       False          142         138      3      NaN      2  \n",
      "38781       False          142         138      3      NaN      2  \n",
      "38782       False          142         138      3      NaN      2  \n",
      "\n",
      "[27687 rows x 40 columns]\n",
      "----------------------------------------------------------\n",
      "Dataset filtré:\n",
      "       score_id  red_penalty  red_point  blue_penalty  blue_point  \\\n",
      "10           11            1          0             0           0   \n",
      "11           12            1          0             0           0   \n",
      "12           13            1          0             0           0   \n",
      "37           38            1          0             0           0   \n",
      "38           39            1          0             0           0   \n",
      "...         ...          ...        ...           ...         ...   \n",
      "38770     38944            0          0             1           0   \n",
      "38771     38945            0          0             1           0   \n",
      "38772     38946            1          0             0           0   \n",
      "38773     38947            1          0             0           0   \n",
      "38774     38948            1          0             0           0   \n",
      "\n",
      "                           date_create                      date_change  \\\n",
      "10    2021-06-26 11:14:57.512104+00:00 2021-06-26 11:14:57.512115+00:00   \n",
      "11    2021-06-26 11:14:59.157967+00:00 2021-06-26 11:14:59.157977+00:00   \n",
      "12    2021-06-26 11:15:02.026617+00:00 2021-06-26 11:15:02.026635+00:00   \n",
      "37    2021-06-26 11:15:35.603208+00:00 2021-06-26 11:15:35.603218+00:00   \n",
      "38    2021-06-26 11:15:36.311695+00:00 2021-06-26 11:15:36.311706+00:00   \n",
      "...                                ...                              ...   \n",
      "38770 2023-04-15 14:18:18.272223+00:00 2023-04-15 14:18:18.272234+00:00   \n",
      "38771 2023-04-15 14:18:18.321244+00:00 2023-04-15 14:18:18.321255+00:00   \n",
      "38772 2023-04-15 14:18:22.517336+00:00 2023-04-15 14:18:22.517346+00:00   \n",
      "38773 2023-04-15 14:18:22.957239+00:00 2023-04-15 14:18:22.957249+00:00   \n",
      "38774 2023-04-15 14:18:23.329038+00:00 2023-04-15 14:18:23.329051+00:00   \n",
      "\n",
      "       judge_id  match_id                  date_create_app  ...  \\\n",
      "10       114816     21089 2021-06-26 11:14:58.645000+00:00  ...   \n",
      "11       114824     21089 2021-06-26 11:14:58.993000+00:00  ...   \n",
      "12       114822     21089 2021-06-26 11:14:59.977000+00:00  ...   \n",
      "37       114816     21089 2021-06-26 11:15:36.724000+00:00  ...   \n",
      "38       114822     21089 2021-06-26 11:15:34.260000+00:00  ...   \n",
      "...         ...       ...                              ...  ...   \n",
      "38770    114926     22514 2023-04-15 14:18:17.865000+00:00  ...   \n",
      "38771    115226     22514 2023-04-15 14:18:18.886000+00:00  ...   \n",
      "38772    115226     22514 2023-04-15 14:18:23.087000+00:00  ...   \n",
      "38773    114927     22514 2023-04-15 14:18:22.818000+00:00  ...   \n",
      "38774    114926     22514 2023-04-15 14:18:22.918000+00:00  ...   \n",
      "\n",
      "      PalmaresRealStartTime  PalmaresRingID  blue_injured           open_time  \\\n",
      "10      2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "11      2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "12      2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "37      2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "38      2023-06-19 11:12:45             1.0         False 2023-06-19 11:12:01   \n",
      "...                     ...             ...           ...                 ...   \n",
      "38770   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38771   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38772   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38773   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "38774   2023-06-19 14:13:13            99.0         False 2023-06-19 14:12:41   \n",
      "\n",
      "      red_injured  blue_height  red_height  round  outlier  phase  \n",
      "10          False          182         179      1      NaN      1  \n",
      "11          False          182         179      1      NaN      1  \n",
      "12          False          182         179      1      NaN      1  \n",
      "37          False          182         179      1      NaN      2  \n",
      "38          False          182         179      1      NaN      2  \n",
      "...           ...          ...         ...    ...      ...    ...  \n",
      "38770       False          142         138      3      NaN      1  \n",
      "38771       False          142         138      3      NaN      1  \n",
      "38772       False          142         138      3      NaN      1  \n",
      "38773       False          142         138      3      NaN      1  \n",
      "38774       False          142         138      3      NaN      1  \n",
      "\n",
      "[7358 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtrage des lignes sans points négatifs dans blue_point et red_point, et sans red_penalty ou blue_penalty\n",
    "dataset = data[(data['blue_point'] >= 0) & (data['red_point'] >= 0) & (data['red_penalty'] == 0) & (data['blue_penalty'] == 0)]\n",
    "\n",
    "# Filtrage des lignes avec des points négatifs dans blue_point et red_point, ou avec red_penalty ou blue_penalty\n",
    "filtered_data = data[(data['blue_point'] < 0) | (data['red_point'] < 0) | (data['red_penalty'] > 0) | (data['blue_penalty'] > 0)]\n",
    "\n",
    "# Affichage du résultat\n",
    "print(\"Dataset sans points négatifs et pénalités:\")\n",
    "print(dataset)\n",
    "\n",
    "# Affichage du résultat\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Dataset filtré:\")\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d72ac",
   "metadata": {},
   "source": [
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il définit une fonction appelée `dtw_distance` qui calcule la distance DTW (Dynamic Time Warping) entre deux séquences de dates. La fonction utilise la bibliothèque `fastdtw` et la distance euclidienne comme mesure de distance.\n",
    "\n",
    "2. Il initialise une liste appelée `final_data` pour stocker les données finales.\n",
    "\n",
    "3. Il itère sur chaque `match_id` unique dans le DataFrame `dataset`.\n",
    "\n",
    "4. Pour chaque `match_id`, il sélectionne les données correspondantes dans le DataFrame `dataset`.\n",
    "\n",
    "5. Il obtient les rounds distincts pour le `match_id` spécifique.\n",
    "\n",
    "6. Pour chaque round, il sélectionne les données du round actuel.\n",
    "\n",
    "7. Il obtient les phases distinctes pour le round actuel.\n",
    "\n",
    "8. Pour chaque phase, il sélectionne les données de la phase actuelle.\n",
    "\n",
    "9. Les données de la phase sont triées par la colonne 'date_create_app' et la colonne 'date_create_app' est convertie en valeurs de temps en millisecondes.\n",
    "\n",
    "10. Il calcule la distance DTW pour chaque paire de lignes consécutives dans la phase en utilisant la fonction `dtw_distance`. La distance est stockée dans la colonne 'dtw_distance' du DataFrame.\n",
    "\n",
    "11. Il calcule l'écart-type de la distance DTW pour la phase actuelle.\n",
    "\n",
    "12. Il effectue la détection des échanges dans la phase en vérifiant certaines conditions. Les informations sur les échanges sont stockées dans la colonne 'echange' du DataFrame.\n",
    "\n",
    "13. Il ajoute les colonnes 'error' et 'judge_id_miss' au DataFrame pour détecter les erreurs et les identifiants de juge manquants dans les échanges.\n",
    "\n",
    "14. Les colonnes 'date_create_app' et 'timestamp_ms' sont converties au format heures, minutes, secondes et millisecondes.\n",
    "\n",
    "15. Les données de la phase sont ajoutées à la liste `final_data`.\n",
    "\n",
    "16. Une fois toutes les phases traitées pour un `match_id`, le code continue avec le prochain `match_id` dans la boucle.\n",
    "\n",
    "17. Une fois toutes les données traitées, les données de toutes les phases sont concaténées dans le DataFrame `final_data`.\n",
    "\n",
    "18. Les données du DataFrame `filtered_data` (résultant du filtrage précédent) sont fusionnées avec `final_data` pour former `merged_data`.\n",
    "\n",
    "19. Les données fusionnées sont exportées dans un fichier CSV appelé \"resultat.csv\".\n",
    "\n",
    "20. Le code affiche \"Fichier exporté.\" pour indiquer que l'exportation a réussi.\n",
    "\n",
    "En résumé, ce code effectue une série de traitements et de calculs sur les données pour détecter les échanges, les erreurs et les identifiants de juge manquants dans les phases des matchs. Les résultats sont ensuite exportés dans un fichier CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cecdd8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier exporté.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour calculer la distance DTW entre deux séquences de dates\n",
    "def dtw_distance(sequence1, sequence2):\n",
    "    sequence1 = np.array(sequence1).reshape(-1, 1)\n",
    "    sequence2 = np.array(sequence2).reshape(-1, 1)\n",
    "    distance, _ = fastdtw(sequence1, sequence2, dist=euclidean)\n",
    "    return distance\n",
    "\n",
    "# Liste pour stocker les données finales\n",
    "final_data = []\n",
    "\n",
    "# Parcours de chaque match_id dans le dataset\n",
    "for match_id in dataset['match_id'].unique():\n",
    "    # Sélection des données correspondant au match_id spécifique\n",
    "    match_data = dataset[dataset['match_id'] == match_id].copy()\n",
    "\n",
    "    # Obtention des rounds distincts pour le match_id spécifique\n",
    "    rounds = match_data['round'].unique()\n",
    "\n",
    "    # Parcours de chaque round\n",
    "    for round_val in rounds:\n",
    "        # Sélection des données du round actuel\n",
    "        round_data = match_data[match_data['round'] == round_val].copy()\n",
    "\n",
    "        # Obtention des phases distinctes pour le round actuel\n",
    "        phases = round_data['phase'].unique()\n",
    "\n",
    "        # Parcours de chaque phase\n",
    "        for phase_val in phases:\n",
    "            # Sélection des données de la phase actuelle\n",
    "            phase_data = round_data[round_data['phase'] == phase_val].copy()\n",
    "\n",
    "            # Tri des données par 'date_create_app'\n",
    "            phase_data.sort_values(by='date_create_app', inplace=True)\n",
    "\n",
    "            # Conversion de la colonne 'date_create_app' en valeurs de temps en millisecondes\n",
    "            phase_data['timestamp_ms'] = phase_data['date_create_app'].view('int64') // 10**6\n",
    "\n",
    "            # Calcul de la distance DTW pour chaque paire de lignes consécutives\n",
    "            phase_data['dtw_distance'] = 100  # Initialisation de la colonne \"dtw_distance\"\n",
    "\n",
    "            for i in range(1, len(phase_data)):\n",
    "                previous_row = phase_data.iloc[i-1]\n",
    "                current_row = phase_data.iloc[i]\n",
    "                previous_sequence = [previous_row['timestamp_ms']]\n",
    "                current_sequence = [current_row['timestamp_ms']]\n",
    "\n",
    "                # Calcul de la distance DTW entre les deux séquences\n",
    "                distance = dtw_distance(previous_sequence, current_sequence)\n",
    "\n",
    "                # Mise à jour de la colonne \"dtw_distance\" avec la distance calculée\n",
    "                phase_data.at[current_row.name, 'dtw_distance'] = distance\n",
    "\n",
    "            # Calcul de l'écart-type de la distance DTW pour la phase actuelle\n",
    "            phase_std = phase_data['dtw_distance'].std()\n",
    "\n",
    "            # Détection des échanges\n",
    "            phase_data['echange'] = 1  # Initialisation de la colonne \"echange\"\n",
    "            echange_number = 1\n",
    "            line_count = 1\n",
    "            unique_judge_ids = set()\n",
    "\n",
    "            for i in range(len(phase_data)):\n",
    "                current_row = phase_data.iloc[i]\n",
    "                current_distance = current_row['dtw_distance']\n",
    "                current_judge_id = current_row['judge_id']\n",
    "                blue_point = current_row['blue_point']\n",
    "                red_point = current_row['red_point']\n",
    "\n",
    "                # Vérifier les conditions pour un nouvel échange\n",
    "                if (current_distance > phase_std or line_count == 3 or\n",
    "                        (line_count > 0 and (blue_point < 0 or red_point < 0)) or\n",
    "                        current_judge_id in unique_judge_ids):  # Nouvel échange si le judge_id est déjà présent\n",
    "                    echange_number += 1\n",
    "                    line_count = 1\n",
    "                    unique_judge_ids = set([current_judge_id])\n",
    "                else:\n",
    "                    line_count += 1\n",
    "                    unique_judge_ids.add(current_judge_id)\n",
    "\n",
    "                phase_data.at[current_row.name, 'echange'] = echange_number\n",
    "\n",
    "            # Ajout des colonnes \"error\" et \"judge_id_miss\"\n",
    "            phase_data['error'] = False  # Initialisation de la colonne \"error\"\n",
    "            phase_data['judge_id_miss'] = ''  # Initialisation de la colonne \"judge_id_miss\"\n",
    "\n",
    "            # Parcours des échanges pour détecter les erreurs et les judge_id manquants\n",
    "            exchanges = phase_data['echange'].unique()\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                exchange_data = phase_data[phase_data['echange'] == exchange].copy()\n",
    "\n",
    "                if len(exchange_data['judge_id'].unique()) == 3:\n",
    "                    # Erreur détectée si trois judge_id différents dans l'échange\n",
    "                    unique_blue_points = exchange_data['blue_point'].unique()\n",
    "                    unique_red_points = exchange_data['red_point'].unique()\n",
    "\n",
    "                    if len(unique_blue_points) > 1 or len(unique_red_points) > 1:\n",
    "                        phase_data.loc[exchange_data.index, 'error'] = False\n",
    "                        if len(unique_blue_points) > 1:\n",
    "                            phase_data.loc[exchange_data[exchange_data['blue_point'] != unique_blue_points[0]].index, 'error'] = True\n",
    "                        if len(unique_red_points) > 1:\n",
    "                            phase_data.loc[exchange_data[exchange_data['red_point'] != unique_red_points[0]].index, 'error'] = True\n",
    "\n",
    "                if len(exchange_data['judge_id'].unique()) == 2:\n",
    "                    # Détecter le judge_id manquant si deux judge_id dans l'échange\n",
    "                    missing_judge_id = set(phase_data['judge_id'].unique()) - set(exchange_data['judge_id'].unique())\n",
    "                    if missing_judge_id:\n",
    "                        phase_data.loc[exchange_data.index, 'judge_id_miss'] = missing_judge_id.pop()\n",
    "\n",
    "            # Conversion de la colonne 'date_create_app' au format heures, minutes, secondes et millisecondes\n",
    "            phase_data['date_create_app'] = pd.to_datetime(phase_data['date_create_app'])\n",
    "            phase_data['timestamp_ms'] = phase_data['date_create_app'].dt.strftime('%H:%M:%S.%f')\n",
    "\n",
    "            # Ajout des données de la phase au tableau final\n",
    "            final_data.append(phase_data)\n",
    "\n",
    "# Concaténation des données de toutes les phases\n",
    "final_data = pd.concat(final_data)\n",
    "\n",
    "# Fusion du dataset avec filtered_data\n",
    "merged_data = pd.concat([final_data, filtered_data])\n",
    "\n",
    "# Exportation des données fusionnées dans un fichier CSV\n",
    "print(\"Fichier exporté.\")\n",
    "merged_data.to_csv('resultat.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4dce0",
   "metadata": {},
   "source": [
    "Ce code effectue les opérations suivantes :\n",
    "\n",
    "1. Il charge deux fichiers CSV : \"resultat.csv\" dans le DataFrame `df_resultat` et \"LCBA_scores.csv\" dans le DataFrame `df_lcba_scores`.\n",
    "\n",
    "2. Il vérifie les \"score_id\" manquants dans le DataFrame `df_resultat` par rapport à ceux présents dans le DataFrame `df_lcba_scores`.\n",
    "\n",
    "3. Il crée un ensemble (`lcba_scores_ids`) contenant tous les \"score_id\" du DataFrame `df_lcba_scores` et un autre ensemble (`resultat_ids`) contenant tous les \"score_id\" du DataFrame `df_resultat`.\n",
    "\n",
    "4. Il détermine les \"score_id\" manquants en effectuant la différence entre `lcba_scores_ids` et `resultat_ids`. Si des \"score_id\" manquants sont détectés (la longueur de `missing_ids` est supérieure à 0), le code continue avec les étapes suivantes. Sinon, il affiche \"Tous les score_id de LCBA_scores se trouvent déjà dans resultat.\" et se termine.\n",
    "\n",
    "5. Il filtre les lignes du DataFrame `df_lcba_scores` qui contiennent les \"score_id\" manquants, créant ainsi un nouveau DataFrame appelé `missing_rows`.\n",
    "\n",
    "6. Il concatène les lignes manquantes (`missing_rows`) avec le DataFrame `df_resultat`, en ignorant les index existants (`ignore_index=True`), pour former un nouveau DataFrame appelé `df_resultat`.\n",
    "\n",
    "7. Il exporte le DataFrame `df_resultat` avec les lignes manquantes dans un nouveau fichier CSV appelé \"resultat_final.csv\" en utilisant `to_csv`.\n",
    "\n",
    "8. Il affiche \"Les lignes des score_id manquants ont été ajoutées dans resultat_final.csv.\" pour indiquer que les lignes manquantes ont été ajoutées avec succès.\n",
    "\n",
    "En résumé, ce code compare les \"score_id\" entre deux fichiers CSV et ajoute les lignes correspondantes manquantes dans le DataFrame `df_resultat`. Le DataFrame mis à jour est ensuite exporté dans un nouveau fichier CSV appelé \"resultat_final.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cdb64a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ray\\AppData\\Local\\Temp\\ipykernel_14836\\3695256590.py:2: DtypeWarning: Columns (40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_resultat = pd.read_csv('resultat.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les lignes des score_id manquants ont été ajoutées dans resultat_final.csv.\n"
     ]
    }
   ],
   "source": [
    "# Charger les deux fichiers CSV\n",
    "df_resultat = pd.read_csv('resultat.csv')\n",
    "df_lcba_scores = pd.read_csv('LCBA_scores.csv')\n",
    "\n",
    "# Vérifier les \"score_id\" manquants dans resultat\n",
    "lcba_scores_ids = set(df_lcba_scores['score_id'])\n",
    "resultat_ids = set(df_resultat['score_id'])\n",
    "\n",
    "missing_ids = lcba_scores_ids - resultat_ids\n",
    "\n",
    "if len(missing_ids) > 0:\n",
    "    # Filtrer les lignes du DataFrame LCBA_scores contenant les \"score_id\" manquants\n",
    "    missing_rows = df_lcba_scores[df_lcba_scores['score_id'].isin(missing_ids)]\n",
    "\n",
    "    # Concaténer les lignes manquantes avec le DataFrame resultat\n",
    "    df_resultat = pd.concat([df_resultat, missing_rows], ignore_index=True)\n",
    "\n",
    "    # Exporter le DataFrame résultat avec les lignes manquantes dans un nouveau fichier CSV\n",
    "    df_resultat.to_csv('resultat_final.csv', index=False)\n",
    "    print(\"Les lignes des score_id manquants ont été ajoutées dans resultat_final.csv.\")\n",
    "else:\n",
    "    print(\"Tous les score_id de LCBA_scores se trouvent déjà dans resultat.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
